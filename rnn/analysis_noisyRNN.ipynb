{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e5c5a6-afa9-44f4-9ea1-3ca5e6024692",
   "metadata": {},
   "source": [
    "# Analysis script for RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89dffb3-4f06-4b6a-8f40-143cb27771c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr as spearmanr\n",
    "from skfda.exploratory.stats import geometric_median as geometric_median\n",
    "\n",
    "# Examine all losses coming from some configuration\n",
    "def extract_run_data(file_seed):\n",
    "    list_files = glob.glob(f'{file_seed}*.pckl')\n",
    "    dict_runs = {}\n",
    "    for ifile, file in enumerate(list_files):\n",
    "        handle = open(file,'rb')\n",
    "        pckl_file = pickle.load(handle)\n",
    "\n",
    "        dict_runs[ifile] = {}\n",
    "        dict_runs[ifile]['run_number'] = int(file.split('run')[1].split('.pckl')[0])\n",
    "        dict_runs[ifile]['loss_total'] = pckl_file['loss_totals']\n",
    "        dict_runs[ifile]['loss_acc_b'] = pckl_file['loss_acc_b']\n",
    "        dict_runs[ifile]['loss_acc_f'] = pckl_file['loss_acc_f']\n",
    "        dict_runs[ifile]['loss_cos_bf'] = pckl_file['loss_cos_bf']\n",
    "        dict_runs[ifile]['pcor_amx_b'] = pckl_file['pcor_amx_b']\n",
    "        dict_runs[ifile]['pcor_amx_f'] = pckl_file['pcor_amx_f']\n",
    "        dict_runs[ifile]['weights_dict'] = pckl_file['weights_dict']\n",
    "        dict_runs[ifile]['cos_penalty'] = pckl_file['cos_penalty']\n",
    "        dict_runs[ifile]['noise_value'] = float(file.split('noise')[1][:3])/100\n",
    "\n",
    "    return dict_runs\n",
    "\n",
    "# Print unique configurations found in directory with training data\n",
    "list_names = glob.glob(\"./saved_data/*.pckl\")\n",
    "list_names = [name[0:-11] for name in list_names]\n",
    "set_names = set(list_names)\n",
    "for name in list(set_names):\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c459f5-8a27-42ae-a5d0-c525a81a4d7f",
   "metadata": {},
   "source": [
    "## Visualize loss trajectories for trained RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4e70d-51f2-4d09-9234-8f6e10b702f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logged loss trajectories \n",
    "''' ---------- INPUT ---------- '''\n",
    "#  [number of hidden units, noise x 100, penalty amount]\n",
    "in_hidden   = 64\n",
    "in_noise    = 1\n",
    "in_penalty  = 40\n",
    "''' -------- END INPUT -------- '''\n",
    "# process numerical to formatted string\n",
    "str_hidden   = f'{in_hidden:03d}'\n",
    "str_noise    = f'{in_noise*100:03d}'\n",
    "str_penalty  = f'{in_penalty:03d}'\n",
    "cfg = ['064', '100', '040'] \n",
    "\n",
    "fileseed = f'./saved_data/n{cfg[0]}_noise{cfg[1]}_cosPnlt_{cfg[2]}'\n",
    "dict_runs = extract_run_data(fileseed)\n",
    "fig, axs = plt.subplots(3, figsize=(6,10), constrained_layout=True)\n",
    "\n",
    "# plot\n",
    "for ifile in range(len(dict_runs)):\n",
    "    # total loss\n",
    "    loss_total_test = dict_runs[ifile]['loss_total'][1]\n",
    "    axs[0].plot(loss_total_test, linewidth=.2, alpha=.6)\n",
    "    axs[0].axhline(loss_total_test[-1], linewidth=.2)\n",
    "    axs[0].set_title('total loss')\n",
    "    # condition wise accuracy loss\n",
    "    loss_acc_b = dict_runs[ifile]['loss_acc_b'][1]\n",
    "    loss_acc_f = dict_runs[ifile]['loss_acc_f'][1]\n",
    "    axs[1].plot(loss_acc_b, linewidth=.2, alpha=.1, color='r')\n",
    "    axs[1].plot(loss_acc_f, linewidth=.2, alpha=.1, color='b')\n",
    "    axs[1].axhline(loss_acc_b[-1], linewidth=.2, color='r')\n",
    "    axs[1].axhline(loss_acc_f[-1], linewidth=.2, color='b')\n",
    "    axs[1].set_title('acc loss')\n",
    "    # visualize cosine sim loss\n",
    "    loss_cos_bf = dict_runs[ifile]['loss_cos_bf'][1]\n",
    "    axs[2].plot(loss_cos_bf, linewidth=.2, alpha=.6)\n",
    "    axs[2].axhline(loss_cos_bf[-1], linewidth=.2)\n",
    "    axs[2].set_title('alignment loss')\n",
    "plt.suptitle(f'{fileseed} | n_runs={len(dict_runs)}');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee50dca-a772-45f3-9cbe-ab3a6be363dc",
   "metadata": {},
   "source": [
    "# Generate an instance of the task to evaluate trained RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e238-4fba-4be5-823d-582b447d671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_noisyRNN import (\n",
    "    eval_noisyRNN as eval_noisyRNN,\n",
    "    generate_blocks as generate_blocks\n",
    ")\n",
    "\n",
    "''' ---------- INPUT ---------- '''\n",
    "n_trials   = 72\n",
    "n_episodes = 6\n",
    "n_blocks   = 500\n",
    "''' -------- END INPUT -------- '''\n",
    "\n",
    "task_list = generate_blocks(n_trials, n_episodes, n_blocks) # task generator for the human task\n",
    "\n",
    "dat_eval = {}\n",
    "for file_seed in set_names:\n",
    "    print(f'\\nProcessing file seed ({file_seed})...')\n",
    "    cfg_str = file_seed.split('/')[2]\n",
    "    if cfg_str[0] != 'n':\n",
    "        continue\n",
    "    nunit_str = cfg_str.split('n')[1][:-1]\n",
    "    nunit = int(nunit_str)\n",
    "    noise_str = cfg_str.split('noise')[1][:3]\n",
    "    cpnlt_str = cfg_str.split('cosPnlt_')[1]\n",
    "    # check if evaluation data dictionary has appropriate key for file seed\n",
    "    if nunit_str not in dat_eval.keys():\n",
    "        dat_eval[nunit_str] = {}\n",
    "    if noise_str not in dat_eval[nunit_str].keys():\n",
    "        dat_eval[nunit_str][noise_str] = {}\n",
    "    if cpnlt_str not in dat_eval[nunit_str][noise_str].keys():\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str] = {}\n",
    "        \n",
    "    dict_runs = extract_run_data(file_seed)\n",
    "    nruns = len(dict_runs)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['run_numbers']  = np.full(nruns, np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['accuracy']     = np.full((nruns, 2), np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['bayesacc']     = np.full((nruns, 2), np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['trace_linear'] = np.full(nruns, np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['trace_quad']   = np.full(nruns, np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['rho_pc_bayes'] = np.full((nruns, 2, nunit), np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['eigvec_b']     = np.full((nunit, nunit, nruns), np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['eigvec_f']     = np.full((nunit, nunit, nruns), np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['cos_dec_bayes']= np.full(nruns, np.nan)\n",
    "    dat_eval[nunit_str][noise_str][cpnlt_str]['cos_enc_bayes']= np.full(nruns, np.nan)\n",
    "\n",
    "    print('Processing run ', end='')\n",
    "    for irun in range(nruns): # choose run index (to be looped over)\n",
    "        print(f'{irun}', end=' ')\n",
    "        \n",
    "        dict_run = dict_runs[irun]\n",
    "        loss_total = dict_run['loss_total'] # get all losses\n",
    "        run_number = dict_run['run_number'] # get run number from filename\n",
    "        \n",
    "        # choosing the epoch \n",
    "        epochs = np.array(list(dict_run['weights_dict'].keys())) # get epochs where weights were saved\n",
    "        # epoch_min_loss = epochs[np.array(loss_total)[1][epochs].argmin()] # choose the epoch corresponding to the minimum total loss\n",
    "        epoch_min_loss = epochs[-1] # choose the epoch corresponding to the minimum total loss\n",
    "        \n",
    "        run_weights = dict_run['weights_dict'][epoch_min_loss] # get the weights for that epoch\n",
    "        run_weights['noise_value'] = dict_run['noise_value'] # get the noise value and store\n",
    "        \n",
    "        # evaluate the network\n",
    "        out = eval_noisyRNN(run_weights, task_list)\n",
    "\n",
    "        # parse output into evaluation dictionary\n",
    "        acc  = out['accuracy'].squeeze().detach().cpu().numpy()\n",
    "        bayesacc  = out['bayesacc'].squeeze().detach().cpu().numpy()\n",
    "        npcs = np.array(out['npcs'])\n",
    "        trace_lin  = np.array(torch.dot(out['trace_components'], out['eig_lin_weights']).detach().cpu().numpy())\n",
    "        trace_quad = np.array(torch.dot(out['trace_components'], out['eig_quad_weights']).detach().cpu().numpy())\n",
    "        corr_pc_bayesenc = out['corr_pc_bayesenc'].detach().cpu().numpy()\n",
    "        V_b = out['eigvecs_b'].detach().cpu().numpy()\n",
    "        V_f = out['eigvecs_f'].detach().cpu().numpy()\n",
    "        cos_dec_bayes = out['cos_dec_bayes'][0].detach().cpu().numpy()\n",
    "        cos_enc_bayes = out['cos_enc_bayes'][0].detach().cpu().numpy()\n",
    "\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['run_numbers'][irun] = dict_run['run_number']\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['accuracy'][irun,:] = acc\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['bayesacc'][irun,:] = bayesacc\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['trace_linear'][irun] = trace_lin\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['trace_quad'][irun] = trace_quad\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['rho_pc_bayes'][irun,:,:] = trace_quad.T\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['eigvec_b'][:,:,irun] = V_b\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['eigvec_f'][:,:,irun] = V_f\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['cos_dec_bayes'][irun] = cos_dec_bayes\n",
    "        dat_eval[nunit_str][noise_str][cpnlt_str]['cos_enc_bayes'][irun] = cos_enc_bayes\n",
    "        \n",
    "print(' ')\n",
    "print('Finished')\n",
    "\n",
    "# save to disk\n",
    "f = open('./saved_data/evaluation_runs.pckl', 'wb')\n",
    "pickle.dump(dat_eval, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79c69e-e520-4df3-aeed-eecd93ac7bf9",
   "metadata": {},
   "source": [
    "## Plots (Figure 4b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb15b02-35d1-4e7b-8f7e-7fcd6c22c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ---------- INPUT ---------- '''\n",
    "save_directory = '../figs/'\n",
    "save_name      = acc_benc_constrained\n",
    "\n",
    "# specify desired model configurations\n",
    "#        units  noise  penalty\n",
    "cfgs = [\n",
    "        ['016', '000', '040'],\n",
    "        ['032', '000', '040'],\n",
    "        ['064', '000', '040'],\n",
    "        ['016', '050', '040'],\n",
    "        ['032', '050', '040'],\n",
    "        ['064', '050', '040'],\n",
    "        ['016', '100', '040'],\n",
    "        ['032', '100', '040'],\n",
    "        ['064', '100', '040'],\n",
    "    \n",
    "        # ['016', '000', 'Non'],\n",
    "        # ['032', '000', 'Non'],\n",
    "        # ['064', '000', 'Non'],\n",
    "        # ['016', '050', 'Non'],\n",
    "        # ['032', '050', 'Non'],\n",
    "        # ['064', '050', 'Non'],\n",
    "        # ['016', '100', 'Non'],\n",
    "        # ['032', '100', 'Non'],\n",
    "        # ['064', '100', 'Non'],\n",
    "       ]\n",
    "\n",
    "''' -------- END INPUT -------- '''\n",
    "\n",
    "# load from disk\n",
    "f = open('./saved_data/evaluation_runs.pckl', 'rb')\n",
    "pckl_file = pickle.load(f)\n",
    "dat_eval = pckl_file\n",
    "\n",
    "''' By default, the y-axis will show accuracy '''\n",
    "# x-axis options\n",
    "is_plotx_trace    = False # plot normalized traces on x-axis\n",
    "is_plotx_cos_benc = False # plot cosine similarity between Bayes enc weights on x-axis\n",
    "# y-axis options\n",
    "is_ploty_benc = False # plot cosine similarity between Bayes enc weights on y-axis\n",
    "is_ploty_bdec = False # plot cosine similarity between Bayes decoding weights on y-axis\n",
    "\n",
    "if is_ploty_benc and is_ploty_bdec:\n",
    "    sys.exit('Both the encoding and decoding weights cannot be simultaneously plotted!')\n",
    "    \n",
    "dat = {}\n",
    "for cfg in cfgs:\n",
    "    # create label\n",
    "    nunit = int(cfg[0])\n",
    "    nsstr = 'n' + cfg[1]\n",
    "    pnstr = cfg[2]\n",
    "    label = f'{nunit}_{nsstr}_{pnstr}'\n",
    "    # store evaluation \n",
    "    dat[label] = dat_eval[cfg[0]][cfg[1]][cfg[2]]\n",
    "fig, axs = plt.subplots(2, figsize=(4,7), constrained_layout=True)\n",
    "fig.set_figwidth(2.4)\n",
    "fig.set_figheight(4.4)\n",
    "for i in range(2):\n",
    "    for ikey, key in enumerate(dat.keys()):\n",
    "        nunits = int(key.split('n')[0][:2])\n",
    "        noisev = int(key.split('n')[1][:3])\n",
    "        \n",
    "        # color for different noise values\n",
    "        match noisev:\n",
    "            case 0:\n",
    "                rgb = [.5,.5,.5]\n",
    "            case 50:\n",
    "                rgb = [.43,.81,.96]\n",
    "            case 100:\n",
    "                rgb = [.43,.61,.96]\n",
    "        match nunits:\n",
    "            case 16:\n",
    "                marker='s' \n",
    "                xval = 1\n",
    "            case 32:\n",
    "                marker='o'\n",
    "                xval = 2\n",
    "            case 64: \n",
    "                marker='d'\n",
    "                xval = 3\n",
    "        dat_tr_q = dat[key]['trace_quad']\n",
    "        dat_tr_l = dat[key]['trace_linear']\n",
    "        dat_acc  = dat[key]['accuracy'][:, i]\n",
    "        dat_bacc = dat[key]['bayesacc'][:, i]\n",
    "        dat_bdec = dat[key]['cos_dec_bayes']\n",
    "        dat_benc = dat[key]['cos_enc_bayes']\n",
    "        dat_tr = dat_tr_l\n",
    "\n",
    "        # data for plots\n",
    "        ydat = dat_acc\n",
    "        ylabelstr = 'Accuracy'\n",
    "        if is_ploty_bdec:\n",
    "            ydat = np.abs(dat_bdec)\n",
    "        if is_ploty_benc:\n",
    "            ydat = np.abs(dat_benc)\n",
    "        if is_ploty_benc or is_ploty_bdec:\n",
    "            ylabelstr = 'Cosine similarity'\n",
    "            for y in np.arange(0, 1, .1):\n",
    "                axs[i].axhline(y, alpha=1, linewidth=.01, c='k')\n",
    "        else:\n",
    "            axs[i].axhline(dat_bacc[0], linewidth=.5, c='k') # bayes\n",
    "            for y in np.arange(.5, .8, .05):\n",
    "                axs[i].axhline(y, alpha=1, linewidth=.01, c='k')\n",
    "            axs[i].set_ylim([.5, .8])\n",
    "\n",
    "        # concatenated data for correlation calculation\n",
    "        if ikey == 0:\n",
    "            dat_all = dat_tr\n",
    "            y_all = ydat\n",
    "        else:\n",
    "            dat_all = np.concatenate((dat_all, dat_tr),axis=None)\n",
    "            y_all = np.concatenate((y_all, ydat),axis=None)\n",
    "            \n",
    "        if is_plotx_trace:\n",
    "            xdat = dat_tr\n",
    "            xlabelstr = 'Normalized trace'\n",
    "        elif is_plotx_cos_benc:\n",
    "            xdat = np.abs(dat_benc)\n",
    "            xlabelstr = 'Cosine similarity'\n",
    "        \n",
    "        if not is_plotx_trace and not is_plotx_cos_benc:\n",
    "            axs[i].errorbar(xval, ydat.mean(), # mean and std\n",
    "                            yerr=ydat.std()/np.sqrt(np.size(ydat)), label=key, marker=marker, c=rgb, ms=7);\n",
    "        else:\n",
    "            # mean and sem\n",
    "            axs[i].errorbar(xdat.mean(), ydat.mean(),\n",
    "                            xerr=dat_tr.std()/np.sqrt(np.size(dat_tr)), yerr=ydat.std()/np.sqrt(np.size(ydat)), label=key, marker=marker, c=rgb, ms=7, mfc='w');\n",
    "            # calculate geometric median\n",
    "            dat_mat = np.column_stack((xdat, ydat))\n",
    "            median = geometric_median(dat_mat)\n",
    "            # geometric median\n",
    "            axs[i].scatter(median[0], median[1], marker=marker, color=rgb, zorder=10, s=10); \n",
    "            for x in np.arange(0, 1, .2):\n",
    "                axs[i].axvline(x, alpha=1, linewidth=.01, c='k')\n",
    "            axs[i].set_xlim([0, 1])\n",
    "            axs[i].set_xlabel(xlabelstr)\n",
    "            axs[i].set_ylabel(ylabelstr)\n",
    "            \n",
    "        # axs[i].legend(loc='lower right', prop={'size': 8})\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=14, labelfontfamily='Arial')\n",
    "        axs[i].tick_params(axis='both', which='minor', labelsize=14, labelfontfamily='Arial')\n",
    "\n",
    "# correlation\n",
    "print(f'{spearmanr(dat_all, y_all).statistic:.3f} p={spearmanr(dat_all, y_all).pvalue:.4f}')\n",
    "\n",
    "fig.savefig(f'./{save_directory}/{save_name}.pdf', format='pdf', bbox_inches = 'tight')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
